from typing import Callable

import torch

from fairsenseai.runtime import FairsenseRuntime,FairsenseCPURuntime, FairsenseGPURuntime

FAIRSENSE_RUNTIME = None

def get_runtime(allow_filesystem_access=True) -> FairsenseRuntime:
    """
    Initializes and returns the global FairsenseRuntime instance.
    Ensures that the runtime is only initialized once.

    Parameters
    ----------
    allow_filesystem_access
        Whether to allow file system access for the runtime.

    Returns
    -------
    FairsenseRuntime
        The FairsenseRuntime instance.
    """
    global FAIRSENSE_RUNTIME
    if FAIRSENSE_RUNTIME is None:
        if torch.cuda.is_available():
            FAIRSENSE_RUNTIME = FairsenseGPURuntime(allow_filesystem_access)
        else:
            FAIRSENSE_RUNTIME = FairsenseCPURuntime(allow_filesystem_access)
    return FAIRSENSE_RUNTIME

def generate_response_with_model(
    prompt: str,
    progress: Callable[[float, str], None] = None
) -> str:
    """
    Higher-level function that calls the configured runtime to generate a response.

    Parameters
    ----------
    prompt
        The input prompt for the text model.
    progress
        A callback function to report progress.

    Returns
    -------
    str
        Text response generated by the model.
    """
    fairsense_runtime = get_runtime()
    try:
        return fairsense_runtime.predict_with_text_model(prompt, progress)
    except Exception as e:
        if progress:
            progress(1.0, "Error occurred")
        return f"Error generating response: {e}"
